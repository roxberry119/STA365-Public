{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b923a2",
   "metadata": {},
   "source": [
    "# Homework 5: Part I\n",
    "\n",
    "1. Go get data from kaggle.com and do a ***Bayesian Linear Regression*** analysis\n",
    "\n",
    "```python\n",
    "import pymc as pm; import numpy as np\n",
    "n,p=100,10; X,y=np.zeros((n,p)),np.ones((n,1))\n",
    "# Replace this made up data with your data set from kaggle...\n",
    "with pm.Model() as MLR:\n",
    "    betas = pm.MvNormal('betas', mu=np.zeros((p,1)), cov=np.eye(p), shape=(p,1))\n",
    "    sigma = pm.TruncatedNormal('sigma', mu=1, sigma=1, lower=0) # half normal\n",
    "    y = pm.Normal('y', mu=pm.math.dot(X, betas), sigma=sigma, observed=y)\n",
    "\n",
    "with MLR:\n",
    "    idata = pm.sample()\n",
    "```    \n",
    "\n",
    "2. Choose ***prior*** that are sensible: certainly you might change the ***hyperparameters***, and perhaps you can experiment with different distributional families for `sigma`...\n",
    "\n",
    "3. [Optional] Assess the performance of the MCMC and note any issues or warnings\n",
    "\n",
    "    1. Traceplots, inference (credible) intervals, effective sample sizes, energy plots, warnings and other notes... just the usual stuff they do [here](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#pymc-overview)\n",
    "\n",
    "4. [Optional] Perform ***Multiple Linear Regression*** diagnostics... residual plots, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c5f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!kaggle datasets download -d titanic\n",
    "!unzip titanic.zip\n",
    "import pandas as pd\n",
    "heart_data = pd.read_csv(\"heart_disease_uci.csv\")\n",
    "heart_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm; import numpy as np\n",
    "\n",
    "# Extract features (X) and target variable (y) from your dataset\n",
    "X = heart_data['age','sex','chol']  # Replace 'target_column_name' with the name of your target variable column\n",
    "y = heart_data['trestbps']\n",
    "\n",
    "n, p = X.shape[0], X.shape[1]; X,y=np.zeros((n,p)),np.ones((n,1))\n",
    "\n",
    "with pm.Model() as MLR:\n",
    "    betas = pm.MvNormal('betas', mu=np.zeros((p,1)), cov=np.eye(p), shape=(p,1))\n",
    "    sigma = pm.TruncatedNormal('sigma', mu=1, sigma=1, lower=0) # half normal\n",
    "    y = pm.Normal('y', mu=pm.math.dot(X, betas), sigma=sigma, observed=y)\n",
    "\n",
    "with MLR:\n",
    "    idata = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e503dc",
   "metadata": {},
   "source": [
    "# Homework 5: Part II\n",
    "    \n",
    "## Answer the following with respect to $p(\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y})$ on the previous slide\n",
    "    \n",
    "1. Rewrite $p(\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y})$ in terms of $\\sigma^2$ (no longer using $\\Sigma$) if $\\Sigma=\\sigma^2I$\n",
    "\n",
    "2. What is $E[\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}]$?\n",
    "\n",
    "3. What ***hyperparameters*** values (legal or illegal) would make $E[\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}] = (\\mathbf{X^\\top X})^{-1}\\mathbf{X^\\top y}$?\n",
    "\n",
    "4. What ***hyperparameters*** values (legal or illegal) would make $E[  \\mathbf{\\hat y} = \\mathbf{X}\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}] = \\mathbf{X}(\\mathbf{X^\\top X})^{-1}\\mathbf{X^\\top y}$?\n",
    "\n",
    "5. What is $\\text{Var}[\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd95cb",
   "metadata": {},
   "source": [
    "# \n",
    "1. Given $\\Sigma=\\sigma^2I$, where I is the identity matrix, we can express the multivariate normal distribution as:\n",
    "    $\\( p(\\boldsymbol \\beta |\\sigma^2, \\mathbf{X},\\mathbf{y}) \\$)\n",
    "    \n",
    "2. The expected value of $\\boldsymbol \\beta$ given $\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}$ is represented by $\\boldsymbol \\mu$. Therefore, $E[\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}] = \\boldsymbol \\mu$.\n",
    "3. To make $E[\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}] = (\\mathbf{X^\\top X})^{-1}\\mathbf{X^\\top y}$, the hyperparameter values would need to set $\\boldsymbol \\mu$ equal to $(\\mathbf{X^\\top X})^{-1}\\mathbf{X^\\top y}$.\n",
    "4. Similarly, to make $E[ \\mathbf{\\hat y} = \\mathbf{X}\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}] = \\mathbf{X}(\\mathbf{X^\\top X})^{-1}\\mathbf{X^\\top y}$, the hyperparameter values would need to set $\\boldsymbol \\mu$ equal to $\\mathbf{X}(\\mathbf{X^\\top X})^{-1}\\mathbf{X^\\top y}$.\n",
    "5. The variance of $\\boldsymbol \\beta$ given $\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}$ is $\\sigma^2 (\\mathbf{X^\\top X})^{-1}$. Therefore, $\\text{Var}[\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}] = \\sigma^2 (\\mathbf{X^\\top X})^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46e55d",
   "metadata": {},
   "source": [
    "# Homework 5: Part III\n",
    "\n",
    "1. Go get data from kaggle.com and perform inference for a ***Bayesian Multivariate Normal Model***\n",
    "\n",
    "<SPAN STYLE=\"font-size:18.0pt\">\n",
    "\n",
    "```python\n",
    "import numpy as np; from scipy import stats\n",
    "p=10; Psi=np.eye(p); a_cov = stats.invwishart(df=p+2, scale=Psi).rvs(1)\n",
    "n=1000; y=stats.multivariate_normal(mean=np.zeros(p), cov=a_cov).rvs(size=n)\n",
    "# Replace this made up data with your data set from kaggle...\n",
    "    \n",
    "with pm.Model() as MNV_LKJ:\n",
    "    packed_L = pm.LKJCholeskyCov(\"packed_L\", n=p, eta=2.0,\n",
    "                                 sd_dist=pm.Exponential.dist(1.0, shape=2), compute_corr=False)\n",
    "    L = pm.expand_packed_triangular(p, packed_L)\n",
    "    # Sigma = pm.Deterministic('Sigma', L.dot(L.T)) # Don't use a covariance matrix parameterization\n",
    "    mu = pm.MvNormal('mu', mu=np.array(0), cov=np.eye(p), shape=p);\n",
    "    # y = pm.MvNormal('y', mu=mu, cov=Sigma, shape=(n,1), observed=y)\n",
    "    # Figure out how to parameterize this with a Cholesky factor to improve computational efficiency\n",
    "with MNV_LKJ    \n",
    "    idata = pm.sample()\n",
    "```    \n",
    "</SPAN>\n",
    "\n",
    "2. As indicated above, don't use a covariance matrix parameterization and instead figure out how to parameterize this with a ***Cholesky factor*** to improve computational efficiency. The ***Cholesky***-based formulation allows general $O(n^3)$ $\\det({\\boldsymbol \\Sigma})$ to be computed using a simple $O(n)$ product and general $O(n^3)$ ${\\boldsymbol \\Sigma}^{-1}$ to be instead evaluated with $O(n^2)$ ***backward substitution***.\n",
    "\n",
    "2. Specify ***priors*** that work: certainly you'll likely need to change the ***prior hyperparameters*** for $\\boldsymbol \\mu$ (`mu`) and $\\mathbf{R}$ (`packed_L`)...\n",
    "    1. And you could consider adjusting the ***prior*** for $\\boldsymbol \\sigma$ using `sd_dist`...\n",
    "\n",
    "3. [Optional] Assess the performance of the MCMC and note any issues\n",
    "\n",
    "    1. Traceplots, inference (credible) intervals, effective sample sizes, energy plots, warnings and other notes... just the usual stuff they do [here](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#pymc-overview)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783e91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "p = 10  # Dimensionality of the multivariate normal\n",
    "Psi = np.eye(p)  # Scale matrix for the Wishart distribution\n",
    "a_cov = stats.invwishart(df=p+2, scale=Psi).rvs(1)  # Inverse-Wishart sample for the covariance matrix\n",
    "\n",
    "n = 1000  # Number of data points\n",
    "y = stats.multivariate_normal(mean=np.zeros(p), cov=a_cov).rvs(size=n)  # Simulated data points\n",
    "\n",
    "# Using PyMC3 to define the model\n",
    "with pm.Model() as MNV_LKJ:\n",
    "    # Define the Cholesky factor of the covariance matrix\n",
    "    packed_L = pm.LKJCholeskyCov(\"packed_L\", n=p, eta=2.0,\n",
    "                                 sd_dist=pm.Exponential.dist(1.0, shape=p), compute_corr=False)\n",
    "    L = pm.expand_packed_triangular(p, packed_L)\n",
    "    Sigma = pm.Deterministic('Sigma', L.dot(L.T))  # Reconstructing the covariance matrix\n",
    "\n",
    "    # Define the prior for the mean vector using a small constant for the covariance for numerical stability\n",
    "    mu = pm.MvNormal('mu', mu=np.zeros(p), cov=np.eye(p)*1e-6, shape=p)\n",
    "\n",
    "    # The observed data likelihood, parameterized using the Cholesky factor\n",
    "    y_obs = pm.MvNormal('y_obs', mu=mu, chol=L, observed=y)\n",
    "\n",
    "# Sampling from the model\n",
    "with MNV_LKJ:\n",
    "    trace = pm.sample(1000)  # You may want to adjust the number of samples\n",
    "\n",
    "# The trace object now contains the samples for the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66bcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
